{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2548ff25-3c6d-4246-9db2-c9dc541add1a",
   "metadata": {},
   "source": [
    "## Download and Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd00315a-8511-4dab-aab4-5338f6bc67c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /opt/mamba/lib/python3.11/site-packages (0.10.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/mamba/lib/python3.11/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /opt/mamba/lib/python3.11/site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/mamba/lib/python3.11/site-packages (from librosa) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/mamba/lib/python3.11/site-packages (from librosa) (1.4.1.post1)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/mamba/lib/python3.11/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/mamba/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/mamba/lib/python3.11/site-packages (from librosa) (0.59.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /opt/mamba/lib/python3.11/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/mamba/lib/python3.11/site-packages (from librosa) (1.8.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/mamba/lib/python3.11/site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/mamba/lib/python3.11/site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /opt/mamba/lib/python3.11/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/mamba/lib/python3.11/site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: packaging in /opt/mamba/lib/python3.11/site-packages (from lazy-loader>=0.1->librosa) (23.2)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/mamba/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/mamba/lib/python3.11/site-packages (from pooch>=1.0->librosa) (4.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/mamba/lib/python3.11/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (3.4.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/mamba/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/mamba/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n",
      "Requirement already satisfied: torch in /opt/mamba/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.11/site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/mamba/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/mamba/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/mamba/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/mamba/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/mamba/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/mamba/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/mamba/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/mamba/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/mamba/lib/python3.11/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/mamba/lib/python3.11/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/mamba/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/mamba/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3d3e58-f1ee-498e-adba-c58dc4d3d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import zipfile\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2c0528-0c01-49df-afd1-13063dd25d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"data/LivingRoom_preprocessed_hack\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5732ff9-2b96-4586-8bed-d864206230ce",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d52680f-4e65-4644-9b2c-253464bfb5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_h1 = np.load(DATASET_PATH + \"/Human1/centroid.npy\")\n",
    "centroids_h2 = np.load(DATASET_PATH + \"/Human2/centroid.npy\")\n",
    "deconvoled_trim_h1 = np.load(DATASET_PATH + \"/Human1/deconvoled_trim.npy\")\n",
    "deconvoled_trim_h2 = np.load(DATASET_PATH + \"/Human2/deconvoled_trim.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251cbcf-b9fa-4ec9-8395-63e2eb431c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deconvoled Trim Human 1 Shape: \", deconvoled_trim_h1.shape)\n",
    "print(\"Deconvoled Trim Human 2 Shape: \", deconvoled_trim_h2.shape)\n",
    "print(\"Centroids Huma 1 Shape: \", centroids_h1.shape)\n",
    "print(\"Centroids Human 2 Shape: \", centroids_h2.shape)\n",
    "print(centroids_h1.shape)\n",
    "print(centroids_h2.shape)\n",
    "print(centroids_h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9aba60-dba1-4f61-9308-e2490d8701a8",
   "metadata": {},
   "source": [
    "## Preprocess Data - Human 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e626fb9f-eaa3-4451-abde-b9aaa1bda2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [13:11<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 44100\n",
    "preprocessed_data_mfcc = []\n",
    "preprocessed_data_rms = []\n",
    "preprocessed_data_zcr = []\n",
    "\n",
    "# Wrap the outer loop with tqdm for progress visualization\n",
    "for instance_index in tqdm(range(deconvoled_trim_h1.shape[0])):\n",
    "    instance_data_mfcc = []\n",
    "    instance_data_zcr = []\n",
    "    instance_data_rms = []\n",
    "    for channel_index in range(deconvoled_trim_h1.shape[1]):\n",
    "        # Filtering\n",
    "        filtered_signal = signal.medfilt(deconvoled_trim_h1[instance_index, channel_index, :], kernel_size=3)\n",
    "        \n",
    "        # Normalization\n",
    "        normalized_signal = librosa.util.normalize(filtered_signal)\n",
    "\n",
    "        # Resampling\n",
    "        resampled_signal = librosa.resample(normalized_signal, orig_sr=sampling_rate, target_sr=16000)\n",
    "        \n",
    "        # Feature extraction\n",
    "        # MFCC features\n",
    "        mfcc_features = librosa.feature.mfcc(y=deconvoled_trim_h1[instance_index, channel_index, :], sr=16000, n_mfcc=13)\n",
    "\n",
    "        # RMS features\n",
    "        rms_features = np.sqrt(np.mean(deconvoled_trim_h1[instance_index, channel_index, :]**2))\n",
    "        # rms_features = librosa.feature.rms(y=normalized_signal)\n",
    "\n",
    "        # Zero-Crossing Rate\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=deconvoled_trim_h1[instance_index, channel_index, :])\n",
    "        \n",
    "        instance_data_mfcc.append(mfcc_features)\n",
    "        instance_data_zcr.append(zero_crossing_rate)\n",
    "        instance_data_rms.append(rms_features)\n",
    "    \n",
    "    preprocessed_data_mfcc.append(instance_data_mfcc)\n",
    "    preprocessed_data_rms.append(instance_data_rms)\n",
    "    preprocessed_data_zcr.append(instance_data_zcr)\n",
    "\n",
    "preprocessed_data_mfcc_h1 = np.array(preprocessed_data_mfcc)\n",
    "preprocessed_data_rms_h1 = np.array(preprocessed_data_rms)\n",
    "preprocessed_data_zcr_h1 = np.array(preprocessed_data_zcr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d3fc2-15d7-425c-aa2b-fe98efd934b6",
   "metadata": {},
   "source": [
    "## Preprocess Data - Human 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc97e08-ff86-485f-b64b-bdd8f6f03cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [02:43<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 44100\n",
    "preprocessed_data_mfcc = []\n",
    "preprocessed_data_rms = []\n",
    "preprocessed_data_zcr = []\n",
    "\n",
    "# Wrap the outer loop with tqdm for progress visualization\n",
    "for instance_index in tqdm(range(deconvoled_trim_h2.shape[0])):\n",
    "    instance_data_mfcc = []\n",
    "    instance_data_zcr = []\n",
    "    instance_data_rms = []\n",
    "    for channel_index in range(deconvoled_trim_h2.shape[1]):\n",
    "        # Filtering\n",
    "        filtered_signal = signal.medfilt(deconvoled_trim_h2[instance_index, channel_index, :], kernel_size=3)\n",
    "        \n",
    "        # Normalization\n",
    "        normalized_signal = librosa.util.normalize(filtered_signal)\n",
    "\n",
    "        # Resampling\n",
    "        resampled_signal = librosa.resample(normalized_signal, orig_sr=sampling_rate, target_sr=16000)\n",
    "        \n",
    "        # Feature extraction\n",
    "        # MFCC features\n",
    "        mfcc_features = librosa.feature.mfcc(y=deconvoled_trim_h2[instance_index, channel_index, :], sr=16000, n_mfcc=13)\n",
    "\n",
    "        # RMS features\n",
    "        rms_features = np.sqrt(np.mean(deconvoled_trim_h2[instance_index, channel_index, :]**2))\n",
    "        # rms_features = librosa.feature.rms(y=normalized_signal)\n",
    "\n",
    "        # Zero-Crossing Rate\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=deconvoled_trim_h2[instance_index, channel_index, :])\n",
    "        \n",
    "        instance_data_mfcc.append(mfcc_features)\n",
    "        instance_data_zcr.append(zero_crossing_rate)\n",
    "        instance_data_rms.append(rms_features)\n",
    "    \n",
    "    preprocessed_data_mfcc.append(instance_data_mfcc)\n",
    "    preprocessed_data_rms.append(instance_data_rms)\n",
    "    preprocessed_data_zcr.append(instance_data_zcr)\n",
    "\n",
    "preprocessed_data_mfcc_h2 = np.array(preprocessed_data_mfcc)\n",
    "preprocessed_data_rms_h2 = np.array(preprocessed_data_rms)\n",
    "preprocessed_data_zcr_h2 = np.array(preprocessed_data_zcr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38576e-7550-4305-bd18-3165c9d62144",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb73b080-5f91-4667-bccb-3dd39868e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, coordinates, preprocessed_data_mfcc=[], _type=\"rms\", preprocessed_data_rms=[], preprocessed_data_zcr=[]):\n",
    "        self.preprocessed_data_mfcc = preprocessed_data_mfcc\n",
    "        self.preprocessed_data_rms = preprocessed_data_rms\n",
    "        self.preprocessed_data_zcr = preprocessed_data_zcr\n",
    "        self.coordinates = coordinates\n",
    "        self.type = _type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.coordinates)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        coordinates = torch.tensor(self.coordinates[idx], dtype=torch.float32)\n",
    "        if self.type == \"rms\":\n",
    "            rms = self.preprocessed_data_rms[idx]\n",
    "            rms = torch.tensor(rms, dtype=torch.float32)\n",
    "            return rms, coordinates\n",
    "        elif self.type == \"mfcc\":\n",
    "            mfcc = [torch.tensor(self.preprocessed_data_mfcc[idx, mic_index], dtype=torch.float32) for mic_index in range(4)]\n",
    "            return mfcc, coordinates\n",
    "        elif self.type == \"zcr\":\n",
    "            zcr = torch.tensor(self.preprocessed_data_zcr[idx], dtype=torch.float32) if self.preprocessed_data_zcr else None\n",
    "            return zcr, coordinates\n",
    "        else:\n",
    "            print(\"Error type\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0712d3e-1ddd-4bde-94f3-a183d50a903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features List Length: 4\n",
      "Features Shape (Microphone 1): tensor(0.0005)\n",
      "Features Shape (Microphone 2): tensor(0.0005)\n",
      "Features Shape (Microphone 3): tensor(0.0007)\n",
      "Features Shape (Microphone 4): tensor(0.0004)\n",
      "Coordinates: tensor([-3198.5410,  -744.5101])\n"
     ]
    }
   ],
   "source": [
    "# Define the custom dataset\n",
    "dataset_h1 = CustomDataset(preprocessed_data_rms=preprocessed_data_rms_h1, coordinates=centroids_h1)\n",
    "dataset_h2 = CustomDataset(preprocessed_data_rms=preprocessed_data_rms_h2, coordinates=centroids_h2)\n",
    "\n",
    "# Access a sample from the dataset\n",
    "features_list, coordinates = dataset_h1[1]\n",
    "print(\"Features List Length:\", len(features_list))\n",
    "print(\"Features Shape (Microphone 1):\", features_list[0])\n",
    "print(\"Features Shape (Microphone 2):\", features_list[1])\n",
    "print(\"Features Shape (Microphone 3):\", features_list[2])\n",
    "print(\"Features Shape (Microphone 4):\", features_list[3])\n",
    "print(\"Coordinates:\", coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf39de3-dd37-4820-81bb-b52f6676ade9",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a9e9b06-0b5d-4da8-8c72-6e5ffce54a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(pred_coords, true_coords):\n",
    "    if not isinstance(pred_coords, np.ndarray):\n",
    "        pred_coords = pred_coords.numpy()\n",
    "    if not isinstance(true_coords, np.ndarray):\n",
    "        pred_coords = true_coords.numpy()\n",
    "    return np.sqrt(np.sum((pred_coords - true_coords)**2))\n",
    "\n",
    "\n",
    "def custom_scoring(estimator, X, y):\n",
    "    # Predict coordinates using the estimator\n",
    "    pred_coords = estimator.predict(X)\n",
    "\n",
    "    # Calculate mean squared error using your euclidean_distance function\n",
    "    ed = np.mean([euclidean_distance(p, t) for p, t in zip(pred_coords, y)])\n",
    "\n",
    "    # Return negative MSE for minimization during grid search\n",
    "    return -ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56144e01-eb31-40f2-9569-2ffa6a2e578e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: RandomForestRegressor(max_depth=15, n_estimators=500, random_state=42)\n",
      "Best Parameters: {'max_depth': 15, 'n_estimators': 500}\n",
      "GridSearchCV Best Scoring: 1375.89115473422\n"
     ]
    }
   ],
   "source": [
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],  # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 15]  # Maximum depth of each tree\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf_regressor, param_grid, cv=5, scoring=custom_scoring) \n",
    "#grid_search = GridSearchCV(rf_regressor, param_grid, cv=5, scoring='neg_mean_squared_error')  # Use negative MSE for regression\n",
    "grid_search.fit(dataset_h1.preprocessed_data_rms, dataset_h1.coordinates)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"GridSearchCV Best Scoring:\", -grid_search.best_score_)  # Negate for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95b3df4f-234b-475e-a3e9-78f1f3df3e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1111374.4260948622\n",
      "Mean Absolute Error: 801.3551807516183\n",
      "Localization Error: 1252.98 (807.95)\n",
      "Score:  0.12698091925025434\n"
     ]
    }
   ],
   "source": [
    "# Train the model if you didn't use GridSearchCV\n",
    "rf_model = RandomForestRegressor(random_state=42, n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'])\n",
    "rf_model.fit(dataset_h1.preprocessed_data_rms, dataset_h1.coordinates)\n",
    "\n",
    "predicted_coords = rf_model.predict(dataset_h2.preprocessed_data_rms)\n",
    "\n",
    "mse = mean_squared_error(centroids_h2, predicted_coords)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "mae = mean_absolute_error(centroids_h2, predicted_coords)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Calculate localization errors for each prediction\n",
    "errors = []\n",
    "for pred_coords, true_coords in zip(predicted_coords, dataset_h2.coordinates):\n",
    "    error = euclidean_distance(pred_coords, true_coords)\n",
    "    errors.append(error)\n",
    "\n",
    "errors = np.array(errors)\n",
    "mean_error = np.mean(errors)\n",
    "stdev_error = np.std(errors)\n",
    "\n",
    "print(\"Localization Error: {:.2f} ({:.2f})\".format(mean_error, stdev_error))\n",
    "print(\"Score: \", rf_model.score(dataset_h2.preprocessed_data_rms, dataset_h2.coordinates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c83e67-0266-43c1-8e33-e5ce0ca556a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
